{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script is to scrape the required data from vlr.gg for model training purpose\n",
    "\n",
    "- navigate to the event page\n",
    "- select event\n",
    "- select matches\n",
    "- scrape the table based on the map\n",
    "- repeat the process until all matches are scrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# other libraries needed\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "options = Options()\n",
    "# options.add_argument('--headless=new') # uncomment this when require to run in headless mode\n",
    "\n",
    "# launch the driver\n",
    "url = \"https://www.vlr.gg/events\" # directly go to event tab (all results are stored in there)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "main_tab_handle = driver.current_window_handle # to locate first tab\n",
    "driver.implicitly_wait(5) # wait for the content to load completely before scrape (5 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Locate/direct to match details and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape function\n",
    "def getMatch(a):\n",
    "    # get the name of the event\n",
    "    event_name = a.find_element(By.CLASS_NAME, 'event-item-title').text\n",
    "    print(f\"scrapping data for {event_name}\", end=\"...\")\n",
    "    # open new tab to show event detail\n",
    "    action = ActionChains(driver)\n",
    "    action.key_down(Keys.CONTROL).click(a).key_up(Keys.CONTROL).perform()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    event_window = driver.current_window_handle\n",
    "    \n",
    "    # navigate to matches tab\n",
    "    nav_bar = driver.find_element(By.CLASS_NAME, 'wf-nav')\n",
    "    matches = nav_bar.find_elements(By.TAG_NAME, 'a') \n",
    "    matches[1].click() # all matches record are stored in matches tab [second tab]\n",
    "    \n",
    "    # click dropdown menu to select \"All Matches\" to display all matches\n",
    "    dropdown_menu_btn = driver.find_element(By.CSS_SELECTOR, '.btn.mod-filter.js-dropdown')\n",
    "    dropdown_menu_btn.click()\n",
    "    time.sleep(2)\n",
    "    driver.find_element(By.CSS_SELECTOR, '.wf-dropdown.mod-all').find_element(By.LINK_TEXT, 'All Stages').click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # all matches are arranges according to date\n",
    "    matches_day = driver.find_elements(By.CLASS_NAME, 'wf-card')\n",
    "    \n",
    "    for day in matches_day[1:]: # first element with class='wf-card' is the event header, skip it\n",
    "        all_matches = day.find_elements(By.TAG_NAME, 'a') # find all clickable element in the div\n",
    "        \n",
    "        for match in all_matches: # find all matches & scrape\n",
    "            # open match details in new tab\n",
    "            action.key_down(Keys.CONTROL).click(match).key_up(Keys.CONTROL).perform()\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(1) \n",
    "            \n",
    "            # locate the maps div\n",
    "            maps = driver.find_element(By.CSS_SELECTOR, \".vm-stats-gamesnav.noselect\").find_elements(By.TAG_NAME, 'div')\n",
    "            \n",
    "            # scrape each maps in the map div\n",
    "            for map in maps:\n",
    "                scrape(map)\n",
    "            \n",
    "            driver.close()# return to previous page and continue next loop\n",
    "            driver.switch_to.window(event_window)\n",
    "            \n",
    "    print(\"done\")\n",
    "    driver.close() # close event window\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_tab_handle) # focus to event list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Scrape the data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(map):\n",
    "    \n",
    "    # TODO: scrape required data from page\n",
    "    \n",
    "    # Access data-disabled and data-game-id attribute value:\n",
    "    disabled_value = map.get_attribute(\"data-disabled\")\n",
    "    game_id_value = map.get_attribute(\"data-game-id\")\n",
    "    \n",
    "    # check the map condition (data-disabled=\"0\") and (data-game-id != \"all\")\n",
    "    if disabled_value == \"0\" and game_id_value != \"all\":\n",
    "        map.click() # switch to respective map\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Steps:\n",
    "    1) scrape the data from the table and insert to a pd dataframe (TBD)\n",
    "    \n",
    "    TBD: how to store the dataset (maybe all 10 players in 1 row)\n",
    "    \n",
    "    2) repeat the steps for each map\n",
    "    '''\n",
    "    \n",
    "    time.sleep(2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate events list and start scrapping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping data for Challengers League 2024 North America: Qualifiers..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# scrape all completed event\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m all_a[:\u001b[38;5;241m1\u001b[39m]: \u001b[38;5;66;03m# remove[:] when need to scrape all data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mgetMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# find page div and go for next page\u001b[39;00m\n\u001b[0;32m     20\u001b[0m page \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction-container-pages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mgetMatch\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# scrape each maps in the map div\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;129;01min\u001b[39;00m maps:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m driver\u001b[38;5;241m.\u001b[39mclose()\u001b[38;5;66;03m# return to previous page and continue next loop\u001b[39;00m\n\u001b[0;32m     44\u001b[0m driver\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mwindow(event_window)\n",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m, in \u001b[0;36mscrape\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m.\u001b[39mclick() \u001b[38;5;66;03m# switch to respective map\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03mSteps:\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m1) scrape the data from the table and insert to a pd dataframe (TBD)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m2) repeat the steps for each map\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize loop condition\n",
    "page = driver.find_element(By.CLASS_NAME, 'action-container-pages')\n",
    "span = page.find_element(By.CSS_SELECTOR, 'span.btn.mod-page.mod-active')\n",
    "next_pages = span.find_elements(By.XPATH, 'following-sibling::a')\n",
    "\n",
    "# while span (selected page) has next page, proceed\n",
    "while len(next_pages) > 0:\n",
    "    # locate completed event div\n",
    "    events_div = driver.find_elements(By.CLASS_NAME, 'events-container-col')\n",
    "    completed_events_div = events_div[1] # completed events always == second div\n",
    "\n",
    "    # extract all clickable element into an array\n",
    "    all_a = completed_events_div.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "    # scrape all completed event\n",
    "    for a in all_a[:1]: # remove[:] when need to scrape all data\n",
    "        getMatch(a)\n",
    "\n",
    "    # find page div and go for next page\n",
    "    page = driver.find_element(By.CLASS_NAME, 'action-container-pages')\n",
    "    span = page.find_element(By.CSS_SELECTOR, 'span.btn.mod-page.mod-active')\n",
    "    next_pages = span.find_elements(By.XPATH, 'following-sibling::a')\n",
    "    \n",
    "    # repeat the process until there is no next page\n",
    "    if len(next_pages) > 0: \n",
    "        print(f'scrapping {span.text}')\n",
    "        next_pages[0].click()\n",
    "    else:\n",
    "        print('completed for all page')\n",
    "\n",
    "# repeat the process for all pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the dataframe as .csv for future use (model training / backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit driver, complete scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit driver after get the html\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
