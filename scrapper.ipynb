{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautidulsoup as scrapper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium for automate scrapping process\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# other libraries needed\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "options = Options()\n",
    "# options.add_argument('--headless=new') # uncomment this when require to run in headless mode\n",
    "\n",
    "# launch the driver\n",
    "url = \"https://www.vlr.gg/events\" # directly go to event tab (all results are stored in there)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "main_tab_handle = driver.current_window_handle # to locate first tab\n",
    "driver.implicitly_wait(5) # wait for the content to load completely before scrape (5 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Locate/direct to match details and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape function\n",
    "def scrape(a):\n",
    "    # open new tab to show event detail\n",
    "    action = ActionChains(driver)\n",
    "    action.key_down(Keys.CONTROL).click(a).key_up(Keys.CONTROL).perform()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    event_window = driver.current_window_handle\n",
    "    \n",
    "    # navigate to matches tab\n",
    "    nav_bar = driver.find_element(By.CLASS_NAME, 'wf-nav')\n",
    "    matches = nav_bar.find_elements(By.TAG_NAME, 'a') \n",
    "    matches[1].click() # all matches record are stored in matches tab [second tab]\n",
    "    matches_day = driver.find_elements(By.CLASS_NAME, 'wf-card')\n",
    "    \n",
    "    for day in matches_day[1:]: # first element with class='wf-card' is the event header, skip it\n",
    "        all_matches = day.find_elements(By.TAG_NAME, 'a') # find all clickable element in the div\n",
    "        \n",
    "        for match in all_matches: # find all matches & scrape\n",
    "            # open match details in new tab\n",
    "            action.key_down(Keys.CONTROL).click(match).key_up(Keys.CONTROL).perform()\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # scrape: TODO\n",
    "            getdata()\n",
    "            \n",
    "            driver.close()# return to previous page and continue next loop\n",
    "            driver.switch_to.window(event_window)\n",
    "            \n",
    "            \n",
    "    driver.close() # close event window\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_tab_handle) # focus to event list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: Scrape the data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    print(\"scrapping data . . .\")\n",
    "    time.sleep(2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate events list and start scrapping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 2\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 3\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 4\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 5\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 6\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 7\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 8\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 9\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 10\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "scrapping data . . .\n",
      "completed for page 11\n",
      "completed for page 12\n",
      "completed for all page\n"
     ]
    }
   ],
   "source": [
    "page = driver.find_element(By.CLASS_NAME, 'action-container-pages')\n",
    "span = page.find_element(By.CSS_SELECTOR, 'span.btn.mod-page.mod-active')\n",
    "next_pages = span.find_elements(By.XPATH, 'following-sibling::a')\n",
    "\n",
    "# while span (selected page) has next page, proceed\n",
    "while len(next_pages) > 0:\n",
    "    # locate completed event div\n",
    "    events_div = driver.find_elements(By.CLASS_NAME, 'events-container-col')\n",
    "    completed_events_div = events_div[1] # completed events always == second div\n",
    "\n",
    "    # extract all clickable element into an array\n",
    "    all_a = completed_events_div.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "    # scrape all completed event\n",
    "    for a in all_a[:1]: # remove[:] when need to scrape all data\n",
    "        scrape(a) # end up back to event list\n",
    "\n",
    "    # find page div and go for next page, repeat the process\n",
    "    page = driver.find_element(By.CLASS_NAME, 'action-container-pages')\n",
    "    span = page.find_element(By.CSS_SELECTOR, 'span.btn.mod-page.mod-active')\n",
    "    next_pages = span.find_elements(By.XPATH, 'following-sibling::a')\n",
    "    if len(next_pages) > 0:\n",
    "        print(f'completed for page {span.text}')\n",
    "        next_pages[0].click()\n",
    "    else:\n",
    "        print('completed for all page')\n",
    "\n",
    "# repeat the process for all pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit driver, complete scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit driver after get the html\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
