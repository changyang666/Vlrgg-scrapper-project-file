{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautidulsoup as scrapper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# selenium for automate scrapping process\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# other libraries needed\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webdriver instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "options = Options()\n",
    "# options.add_argument('--headless=new') # uncomment this when require to run in headless mode\n",
    "\n",
    "# launch the driver\n",
    "url = \"https://www.vlr.gg/events\" # directly go to event tab (all results are stored in there)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "main_tab_handle = driver.current_window_handle # to locate first tab\n",
    "driver.implicitly_wait(5) # wait for the content to load completely before scrape (5 sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with web content/ Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape function\n",
    "def scrape(a):\n",
    "    # open new tab to show event detail\n",
    "    action = ActionChains(driver)\n",
    "    action.key_down(Keys.CONTROL).click(a).key_up(Keys.CONTROL).perform()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    event_window = driver.current_window_handle\n",
    "    \n",
    "    # navigate to matches tab\n",
    "    nav_bar = driver.find_element(By.CLASS_NAME, 'wf-nav')\n",
    "    matches = nav_bar.find_elements(By.TAG_NAME, 'a') \n",
    "    matches[1].click() # all matches record are stored in matches tab [second tab]\n",
    "    matches_day = driver.find_elements(By.CLASS_NAME, 'wf-card')\n",
    "    \n",
    "    for day in matches_day[1:]: # first element with class='wf-card' is the event header, skip it\n",
    "        all_matches = day.find_elements(By.TAG_NAME, 'a') # find all clickable element in the div\n",
    "        \n",
    "        for match in all_matches: # find all matches & scrape\n",
    "            # open match details in new tab\n",
    "            action.key_down(Keys.CONTROL).click(match).key_up(Keys.CONTROL).perform()\n",
    "            driver.switch_to.window(driver.window_handles[2])\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # scrape: TODO\n",
    "            \n",
    "            driver.close()# return to previous page and continue next loop\n",
    "            driver.switch_to.window(event_window)\n",
    "            \n",
    "            \n",
    "    driver.close() # close event window\n",
    "    time.sleep(3)\n",
    "    driver.switch_to.window(main_tab_handle) # focus to event list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event-container --> .wf-label .mod-large .mod-completed text == \"completed events\" to locate completed event div\n",
    "events_div = driver.find_elements(By.CLASS_NAME, 'events-container-col')\n",
    "completed_events_div = events_div[1] # completed events always == second div\n",
    "\n",
    "# extract all clickable element into an array\n",
    "all_a = completed_events_div.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "# scrape all completed event\n",
    "for a in all_a[:2]:\n",
    "    scrape(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit driver, complete scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit driver after get the html\n",
    "# driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
